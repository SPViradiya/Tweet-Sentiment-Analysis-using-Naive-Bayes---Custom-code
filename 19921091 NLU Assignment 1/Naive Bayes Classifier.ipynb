{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.03%\n",
      "Precision: 78.73%\n",
      "Recall: 79.03%\n",
      "NBClassifier Ran in 15.53 seconds\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Classifier \n",
    "#Sandipbhai Pravinbhai Viradiya\n",
    "#Student ID: 19921091\n",
    "\n",
    "from collections import defaultdict #Loading defaultdict from collection for initilizations\n",
    "import pandas as pd # Load the Pandas libraries with alias 'pd' \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#NaiveBayesClassifier Start\n",
    "\n",
    "# Classifier to give positive or negative class of Tweet\n",
    "class NaiveBayesClassifier(object):\n",
    "    #Constructor\n",
    "    def __init__(self):\n",
    "        self.logprior = {} # Variable to store log prior for both of the class\n",
    "        self.loglikelihoog = {} # Variable to store log likelihood for both of the class\n",
    "        self.prohibitExp = [\"&quot;\",\"&lt;\",\"&gt;\"] # Remove double quote, >, < HTML Special chars\n",
    "        self.all_classes = [] # Classes in which we are gonna classify, Here it will be 0 and 1\n",
    "        self.all_voc = [] # All words from all documents\n",
    "         \n",
    "    #Clean the tweet\n",
    "    def processText(self, tweet):\n",
    "        #Convert to lower case\n",
    "        tweet = tweet.lower()\n",
    "\n",
    "        #Convert www.* or https?://* to URL\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "\n",
    "        #Convert @username to AT_USER\n",
    "        tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "\n",
    "        #Remove additional white spaces\n",
    "        tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "\n",
    "        #Replace #word with word\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "        #trim\n",
    "        tweet = tweet.strip('\\'\"')\n",
    "\n",
    "        #remove multiple dots\n",
    "        tweet = re.sub(r'\\.+', \" \", tweet)\n",
    "\n",
    "        #remove multiple dashes\n",
    "        tweet = re.sub(r'\\-+', \" \", tweet)\n",
    "\n",
    "        #replace &quot; &lt; &gt;\n",
    "        for pw in self.prohibitExp:\n",
    "            tweet = tweet.replace(pw,\"\")\n",
    "\n",
    "        #Tokenize string\n",
    "        tokens = nltk.word_tokenize(tweet)\n",
    "\n",
    "        all_tokens = [tok for tok in tokens if tok not in string.punctuation and tok not in [\"AT_USER\",\"URL\"]]\n",
    "\n",
    "        #Remove words like 's or 'll or 'it.\n",
    "        #Remove single quote before word like we have 'without, 'and etc.\n",
    "        for k,tok in enumerate(all_tokens):\n",
    "            if re.match(r\"\\'[A-Za-z0-9]{1,2}$\",tok) or tok == \"\":\n",
    "                all_tokens.remove(tok)\n",
    "            elif re.match(r\"\\'[A-Za-z0-9]{3,}\",tok):\n",
    "                all_tokens[k] = tok.replace(\"'\",\"\")\n",
    "\n",
    "        return all_tokens\n",
    "    \n",
    "    #Count n_c for prior for each class\n",
    "    #Count frequency of each word for each class\n",
    "    #Will return both\n",
    "    def countWordInClasses(self, training_set, training_labels):\n",
    "        cnt = {}\n",
    "        n_c = {}\n",
    "        \n",
    "        # We will store all words here... \n",
    "        #in set, if we use add method, duplicate entries will get automatically replaced\n",
    "        voc = set() \n",
    "\n",
    "        for c in self.all_classes:\n",
    "            cnt[c] = defaultdict(int)\n",
    "            n_c[c] = 0\n",
    "\n",
    "        for doc, doc_label in zip(training_set, training_labels):\n",
    "            n_c[doc_label] += 1\n",
    "            ind_token = self.processText(doc)\n",
    "            for tkn in ind_token:\n",
    "                voc.add(tkn)\n",
    "                cnt[doc_label][tkn] += 1\n",
    "\n",
    "        return cnt, n_c, voc\n",
    "    \n",
    "    #Train using training set and training labels\n",
    "    def train(self, training_set, training_labels, alpha = 1):\n",
    "        #Get number of documents\n",
    "        n_doc = len(training_set)\n",
    "\n",
    "        #Unique classes\n",
    "        self.all_classes = set(training_labels)\n",
    "\n",
    "        # all_words_with_count is frequency of each word with class\n",
    "        # n_c is number of word class wise to use in log-prior\n",
    "        # self.all_voc contains all unique words\n",
    "        all_words_with_count, n_c, self.all_voc = self.countWordInClasses(training_set, training_labels)\n",
    "        total_words = len(self.all_voc) # We need this in denominator for log-likelihood\n",
    "\n",
    "        #print(n_c)\n",
    "        #print(all_words_with_count)\n",
    "        for c in self.all_classes:\n",
    "            \n",
    "            # log-prior for this class\n",
    "            self.logprior[c] = np.log(n_c[c] / n_doc)\n",
    "            \n",
    "            # compute log likekihood of this class\n",
    "            self.loglikelihoog[c] = defaultdict(defaultdict)\n",
    "\n",
    "            # compute words in this class\n",
    "            total_cnt_in_class = 0\n",
    "            for word in all_words_with_count[c]:\n",
    "                total_cnt_in_class += all_words_with_count[c][word]\n",
    "            \n",
    "             # compute the log-likelihood for this class, for every word\n",
    "            for word in self.all_voc:\n",
    "                self.loglikelihoog[c][word] = np.log( (all_words_with_count[c][word] + alpha) / (total_cnt_in_class + ( alpha * total_words )) )\n",
    "     \n",
    "    #Predict if tweet if positive or negative from trained data\n",
    "    #Here test_labels can be optional\n",
    "    #If test_labels IS PASSED, it will return accuracy, precision and recall\n",
    "    #If test_labels IS NOT PASSED, it will return document with predicted sentiment\n",
    "    def predict(self, test_data, test_labels = []):\n",
    "\n",
    "        pred_list = []\n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "\n",
    "        for doc in test_data:\n",
    "            pred = 1 # By default it is positive\n",
    "            \n",
    "            sums = {}\n",
    "            for c in self.all_classes:\n",
    "                sums[c] = 0\n",
    "            \n",
    "            proc_text = self.processText(doc)\n",
    "\n",
    "            for c in sums:\n",
    "                sums[c] = self.logprior[c]\n",
    "                for word in proc_text:\n",
    "                    if word in self.loglikelihoog[c]:\n",
    "                        sums[c] += self.loglikelihoog[c][word]\n",
    "\n",
    "            if sums[0] >= sums[1]:\n",
    "                pred = 0\n",
    "\n",
    "            pred_list.append(pred)\n",
    "\n",
    "        if len(test_labels) > 0:\n",
    "            for real, system in zip(test_labels,pred_list):\n",
    "                if real == 1 and system == 1:\n",
    "                    true_pos += 1\n",
    "                elif real == 1 and system == 0:\n",
    "                    false_neg += 1\n",
    "                elif real == 0 and system == 1:\n",
    "                    false_pos += 1\n",
    "                elif real == 0 and system == 0:\n",
    "                    true_neg += 1\n",
    "\n",
    "            accuracy = ( true_pos + true_neg ) / len(test_labels) * 100\n",
    "            precision = true_pos / ( true_pos + false_pos ) * 100\n",
    "            recall = true_pos / ( true_pos + false_neg ) * 100\n",
    "            print(\"Accuracy: \", round(accuracy,2), \"%\",sep=\"\")\n",
    "            print(\"Precision: \", round(precision,2), \"%\",sep=\"\")\n",
    "            print(\"Recall: \", round(recall,2), \"%\",sep=\"\")\n",
    "        else:\n",
    "            for data, system in zip(test_data,pred_list):\n",
    "                print(data.strip(), ' \"',system, '\" ', sep = \"\")\n",
    "            \n",
    "#NaiveBayesClassifier End\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_set = []\n",
    "all_labels = []\n",
    "training_set = []\n",
    "training_labels = []\n",
    "validation_set = []\n",
    "validation_labels = []\n",
    "\n",
    "# Note for Prof. Manas\n",
    "# Here encoding style is UTF-8 in file but for me ISO-8859-1 words to read it.\n",
    "# Please change it to UTF-8 if provided one not work in your system\n",
    "data = pd.read_csv(\"train.csv\",encoding=\"ISO-8859-1\")\n",
    "for index, row in data.iterrows():\n",
    "    all_set.append(row['SentimentText'])\n",
    "    all_labels.append(row['Sentiment'])\n",
    "\n",
    "#Divide data into 80% Train and 80% Test dataset\n",
    "training_set, validation_set, training_labels, validation_labels = train_test_split(all_set, all_labels, test_size=0.2)\n",
    "\n",
    "#Track time for our train and predict algo\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#Initialization of Naive Bayes Classifier\n",
    "NBClassifier = NaiveBayesClassifier()\n",
    "\n",
    "#Train our model on training set and labels\n",
    "NBClassifier.train(training_set, training_labels, alpha=1)\n",
    "\n",
    "#Predict sentiment from given document\n",
    "#Here if validation labels is not passed, it will return document with sentiment ELSE it will return accuracy, precision and recall percentage\n",
    "NBClassifier.predict(validation_set, validation_labels)\n",
    "\n",
    "# We can only pass document to get predicted Sentinemt\n",
    "#docs = [\"waahhh now I'm getting sad....miss hub :-'(&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\",\"Rose and ood will be back in the Xmas Who special!  YAY!  Damn that's half a year away.\"]\n",
    "#NBClassifier.predict(docs)\n",
    "    \n",
    "end = time.time()\n",
    "print('NBClassifier Ran in {} seconds'.format(round(end - start, 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
